# conv.py

import numpy as np
import matplotlib.pyplot as plt
from collections import OrderedDict


def im2col(input_data, filter_h, filter_w, stride=1, pad=1):
    """
    å°†4Dè¾“å…¥æ•°æ®è½¬æ¢ä¸º2DçŸ©é˜µ
    """
    N, C, H, W = input_data.shape
    out_h = (H + 2 * pad - filter_h) // stride + 1
    out_w = (W + 2 * pad - filter_w) // stride + 1

    # å¡«å……è¾“å…¥æ•°æ®
    img = np.pad(input_data,
                 [(0, 0), (0, 0), (pad, pad), (pad, pad)],
                 'constant')

    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))

    for y in range(filter_h):
        y_max = y + stride * out_h
        for x in range(filter_w):
            x_max = x + stride * out_w
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]

    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)
    return col


def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=1):
    """
    å°†2DçŸ©é˜µè½¬æ¢å›4Dæ•°æ®
    """
    N, C, H, W = input_shape
    out_h = (H + 2 * pad - filter_h) // stride + 1
    out_w = (W + 2 * pad - filter_w) // stride + 1

    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)

    img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))

    for y in range(filter_h):
        y_max = y + stride * out_h
        for x in range(filter_w):
            x_max = x + stride * out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]

    return img[:, :, pad:H + pad, pad:W + pad]


def softmax(x):
    """Softmaxå‡½æ•°"""
    if x.ndim == 2:
        x = x - x.max(axis=1, keepdims=True)
        x = np.exp(x)
        x /= x.sum(axis=1, keepdims=True)
    elif x.ndim == 1:
        x = x - np.max(x)
        x = np.exp(x) / np.sum(np.exp(x))
    return x


def cross_entropy_error(y, t):
    """äº¤å‰ç†µè¯¯å·®"""
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]

    # å¤„ç†one-hotç¼–ç æ ‡ç­¾
    if t.ndim == 2 and t.shape[1] > 1:
        t = t.argmax(axis=1)

    # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°ç±»å‹
    t = t.astype(np.int64).flatten()

    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size


def relu(x):
    """ReLUæ¿€æ´»å‡½æ•°"""
    return np.maximum(0, x)


def relu_grad(x):
    """ReLUæ¢¯åº¦"""
    grad = np.zeros_like(x)
    grad[x > 0] = 1
    return grad


class BatchNormalization:
    """æ‰¹å½’ä¸€åŒ–å±‚"""

    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None
        self.running_mean = running_mean
        self.running_var = running_var
        self.batch_size = None
        self.xc = None
        self.xn = None
        self.std = None
        self.dgamma = None
        self.dbeta = None

    def forward(self, x, train_flg=True):
        self.input_shape = x.shape

        # å¤„ç†4Då·ç§¯æ•°æ®
        if x.ndim == 4:
            N, C, H, W = x.shape
            x = x.transpose(0, 2, 3, 1).reshape(-1, C)
            out = self._forward_2d(x, train_flg)
            out = out.reshape(N, H, W, C).transpose(0, 3, 1, 2)
            return out
        else:
            return self._forward_2d(x, train_flg)

    def _forward_2d(self, x, train_flg):
        if self.running_mean is None:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)

        if train_flg:
            mu = x.mean(axis=0)
            xc = x - mu
            var = np.mean(xc ** 2, axis=0)
            std = np.sqrt(var + 1e-7)
            xn = xc / std

            self.batch_size = x.shape[0]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var
        else:
            xc = x - self.running_mean
            std = np.sqrt(self.running_var + 1e-7)
            xn = xc / std

        out = self.gamma * xn + self.beta
        return out

    def backward(self, dout):
        if dout.ndim == 4:
            N, C, H, W = dout.shape
            dout = dout.transpose(0, 2, 3, 1).reshape(-1, C)
            dx = self._backward_2d(dout)
            dx = dx.reshape(N, H, W, C).transpose(0, 3, 1, 2)
        else:
            dx = self._backward_2d(dout)

        return dx

    def _backward_2d(self, dout):
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xn * dout, axis=0)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)
        dvar = 0.5 * dstd / self.std
        dxc += (2.0 / self.batch_size) * self.xc * dvar
        dmu = np.sum(dxc, axis=0)
        dx = dxc - dmu / self.batch_size

        self.dgamma = dgamma
        self.dbeta = dbeta
        return dx


class Convolution:
    """å·ç§¯å±‚"""

    def __init__(self, W, b, stride=1, pad=1):
        self.W = W  # æ»¤æ³¢å™¨æƒé‡ (FN, C, FH, FW)
        self.b = b  # åç½® (FN,)
        self.stride = stride
        self.pad = pad
        self.x = None
        self.col = None
        self.col_W = None
        self.dW = None
        self.db = None

    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        OH = (H + 2 * self.pad - FH) // self.stride + 1
        OW = (W + 2 * self.pad - FW) // self.stride + 1

        # å°†è¾“å…¥å’Œæ»¤æ³¢å™¨è½¬æ¢ä¸º2DçŸ©é˜µ
        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T

        # å·ç§¯è®¡ç®—
        out = np.dot(col, col_W) + self.b
        out = out.reshape(N, OH, OW, -1).transpose(0, 3, 1, 2)  # (N, FN, OH, OW)

        self.x = x
        self.col = col
        self.col_W = col_W

        return out

    def backward(self, dout):
        FN, C, FH, FW = self.W.shape
        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)

        self.db = np.sum(dout, axis=0)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)

        dcol = np.dot(dout, self.col_W.T)
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)

        return dx


class Pooling:
    """æ± åŒ–å±‚"""

    def __init__(self, pool_h, pool_w, stride=2, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        self.x = None
        self.arg_max = None

    def forward(self, x):
        N, C, H, W = x.shape
        OH = (H - self.pool_h) // self.stride + 1
        OW = (W - self.pool_w) // self.stride + 1

        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h * self.pool_w)

        arg_max = np.argmax(col, axis=1)
        out = np.max(col, axis=1)
        out = out.reshape(N, OH, OW, C).transpose(0, 3, 1, 2)

        self.x = x
        self.arg_max = arg_max
        return out

    def backward(self, dout):
        dout = dout.transpose(0, 2, 3, 1)
        pool_size = self.pool_h * self.pool_w

        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,))

        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)

        return dx


class Affine:
    """å…¨è¿æ¥å±‚"""

    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.original_shape = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.original_shape = x.shape
        x = x.reshape(x.shape[0], -1)
        self.x = x
        out = np.dot(self.x, self.W) + self.b
        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        dx = dx.reshape(self.original_shape)
        return dx


class Relu:
    """ReLUæ¿€æ´»å±‚"""

    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0
        return out

    def backward(self, dout):
        dout[self.mask] = 0
        return dout


class SoftmaxWithLoss:
    """Softmaxä¸æŸå¤±å±‚"""

    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)

        # å¤„ç†æ ‡ç­¾æ ¼å¼
        if self.t.ndim == 2 and self.t.shape[1] > 1:
            self.t = self.t.argmax(axis=1)#666ä½¿ç”¨ç¡¬æ ‡ç­¾ç‰ˆcutmix
        self.t = self.t.astype(np.int64).flatten()

        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = self.y.copy()
        dx[np.arange(batch_size), self.t] -= 1
        dx = dx / batch_size
        return dx


class Dropout:
    """Dropoutå±‚"""

    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)

    def backward(self, dout):
        return dout * self.mask


class Adam:
    """Adamä¼˜åŒ–å™¨"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.iter = 0
        self.m = None
        self.v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1

        for key in params.keys():
            if key in grads:
                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)

                # åç½®æ ¡æ­£
                m_hat = self.m[key] / (1 - self.beta1 ** self.iter)
                v_hat = self.v[key] / (1 - self.beta2 ** self.iter)

                params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)


class SimpleConvNet:
    """ç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œ"""

    def __init__(self, input_dim=(3, 64, 64),
                 conv1_param={'filter_num': 16, 'filter_size': 3, 'pad': 1, 'stride': 1},
                 conv2_param={'filter_num': 32, 'filter_size': 3, 'pad': 1, 'stride': 1},
                 hidden_size=100, output_size=10,
                 use_batchnorm=False, dropout_ratio=0.5):

        self.use_batchnorm = use_batchnorm
        self.dropout_ratio = dropout_ratio

        # å·ç§¯å‚æ•°
        FN1 = conv1_param['filter_num']
        FS1 = conv1_param['filter_size']
        FP1 = conv1_param['pad']
        FSTR1 = conv1_param['stride']

        FN2 = conv2_param['filter_num']
        FS2 = conv2_param['filter_size']
        FP2 = conv2_param['pad']
        FSTR2 = conv2_param['stride']

        # è®¡ç®—å„å±‚è¾“å‡ºå°ºå¯¸
        input_size = input_dim[1]
        conv1_out = (input_size + 2 * FP1 - FS1) // FSTR1 + 1
        pool1_out = conv1_out // 2
        conv2_out = (pool1_out + 2 * FP2 - FS2) // FSTR2 + 1
        pool2_out = conv2_out // 2
        fc_input_size = FN2 * pool2_out * pool2_out

        print("ç½‘ç»œç»“æ„è®¡ç®—:")
        print(f"è¾“å…¥: {input_dim}")
        print(f"å·ç§¯1è¾“å‡º: {FN1} x {conv1_out} x {conv1_out}")
        print(f"æ± åŒ–1è¾“å‡º: {FN1} x {pool1_out} x {pool1_out}")
        print(f"å·ç§¯2è¾“å‡º: {FN2} x {conv2_out} x {conv2_out}")
        print(f"æ± åŒ–2è¾“å‡º: {FN2} x {pool2_out} x {pool2_out}")
        print(f"å…¨è¿æ¥è¾“å…¥: {fc_input_size}")

        # åˆå§‹åŒ–æƒé‡å‚æ•°
        self.params = {}

        # å·ç§¯å±‚æƒé‡ - Heåˆå§‹åŒ–
        he_std1 = np.sqrt(2.0 / (input_dim[0] * FS1 * FS1))
        self.params['W1'] = he_std1 * np.random.randn(FN1, input_dim[0], FS1, FS1)
        self.params['b1'] = np.zeros(FN1)

        he_std2 = np.sqrt(2.0 / (FN1 * FS2 * FS2))
        self.params['W2'] = he_std2 * np.random.randn(FN2, FN1, FS2, FS2)
        self.params['b2'] = np.zeros(FN2)

        # å…¨è¿æ¥å±‚æƒé‡
        he_std3 = np.sqrt(2.0 / fc_input_size)
        self.params['W3'] = he_std3 * np.random.randn(fc_input_size, hidden_size)
        self.params['b3'] = np.zeros(hidden_size)

        he_std4 = np.sqrt(2.0 / hidden_size)
        self.params['W4'] = he_std4 * np.random.randn(hidden_size, output_size)
        self.params['b4'] = np.zeros(output_size)

        # BatchNormå‚æ•°
        if use_batchnorm:
            self.params['gamma1'] = np.ones(FN1)
            self.params['beta1'] = np.zeros(FN1)
            self.params['gamma2'] = np.ones(FN2)
            self.params['beta2'] = np.zeros(FN2)
            self.params['gamma3'] = np.ones(hidden_size)
            self.params['beta3'] = np.zeros(hidden_size)

        # æ„å»ºç½‘ç»œå±‚
        self.layers = OrderedDict()

        # ç¬¬ä¸€å·ç§¯å—
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], FSTR1, FP1)
        if use_batchnorm:
            self.layers['BatchNorm1'] = BatchNormalization(self.params['gamma1'], self.params['beta1'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(2, 2, 2)

        # ç¬¬äºŒå·ç§¯å—
        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'], FSTR2, FP2)
        if use_batchnorm:
            self.layers['BatchNorm2'] = BatchNormalization(self.params['gamma2'], self.params['beta2'])
        self.layers['Relu2'] = Relu()
        self.layers['Pool2'] = Pooling(2, 2, 2)

        # å…¨è¿æ¥å±‚
        self.layers['Affine1'] = Affine(self.params['W3'], self.params['b3'])
        if use_batchnorm:
            self.layers['BatchNorm3'] = BatchNormalization(self.params['gamma3'], self.params['beta3'])
        self.layers['Relu3'] = Relu()
        self.layers['Dropout'] = Dropout(dropout_ratio)
        self.layers['Affine2'] = Affine(self.params['W4'], self.params['b4'])

        self.last_layer = SoftmaxWithLoss()

    def predict(self, x, train_flg=False):
        """å‰å‘ä¼ æ’­é¢„æµ‹"""
        for key, layer in self.layers.items():
            if 'BatchNorm' in key or 'Dropout' in key:
                x = layer.forward(x, train_flg)
            else:
                x = layer.forward(x)
        return x

    def loss(self, x, t):
        """è®¡ç®—æŸå¤±"""
        y = self.predict(x, train_flg=True)
        return self.last_layer.forward(y, t)

    def accuracy(self, x, t, batch_size=100):
        """è®¡ç®—å‡†ç¡®ç‡"""
        if t.ndim != 1:
            t = np.argmax(t, axis=1)

        if x.shape[0] < batch_size:
            batch_size = x.shape[0]

        acc = 0.0
        for i in range(0, x.shape[0], batch_size):
            tx = x[i:i + batch_size]
            tt = t[i:i + batch_size]# çœŸå®æ ‡ç­¾
            y = self.predict(tx, train_flg=False)# é¢„æµ‹
            y = np.argmax(y, axis=1)
            acc += np.sum(y == tt)

        return acc / x.shape[0]

    def gradient(self, x, t):
        """è®¡ç®—æ¢¯åº¦"""
        # å‰å‘ä¼ æ’­
        self.loss(x, t)

        # åå‘ä¼ æ’­
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()#åè½¬åˆ—è¡¨
        for layer in layers:
            dout = layer.backward(dout)

        # æ”¶é›†æ¢¯åº¦
        grads = {}
        grads['W1'] = self.layers['Conv1'].dW
        grads['b1'] = self.layers['Conv1'].db
        grads['W2'] = self.layers['Conv2'].dW
        grads['b2'] = self.layers['Conv2'].db
        grads['W3'] = self.layers['Affine1'].dW
        grads['b3'] = self.layers['Affine1'].db
        grads['W4'] = self.layers['Affine2'].dW
        grads['b4'] = self.layers['Affine2'].db

        if self.use_batchnorm:
            grads['gamma1'] = self.layers['BatchNorm1'].dgamma
            grads['beta1'] = self.layers['BatchNorm1'].dbeta
            grads['gamma2'] = self.layers['BatchNorm2'].dgamma
            grads['beta2'] = self.layers['BatchNorm2'].dbeta
            grads['gamma3'] = self.layers['BatchNorm3'].dgamma
            grads['beta3'] = self.layers['BatchNorm3'].dbeta

        return grads


class Trainer:
    """åªæœ‰è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„è®­ç»ƒå™¨"""

    def __init__(self, network, x_train, t_train, x_val, t_val,
                 epochs=20, batch_size=100, optimizer='adam', learning_rate=0.001,
                 use_augmentation=True, use_cutmix=True, cutmix_alpha=1.0, cutmix_prob=0.5, patience=100,
                 aug_start_threshold=0.85):  # æ–°å¢ï¼šæ•°æ®å¢å¼ºå¯åŠ¨é˜ˆå€¼

        self.network = network
        self.x_train = x_train
        self.t_train = t_train
        self.x_val = x_val
        self.t_val = t_val
        self.patience = patience

        # æ—©åœç›¸å…³å±æ€§
        self.best_val_acc = 0.0
        self.no_improve_count = 0
        self.best_params = None

        # æ•°æ®å¢å¼ºæ§åˆ¶
        self.use_augmentation = use_augmentation
        self.use_cutmix = use_cutmix
        self.aug_start_threshold = aug_start_threshold  # æ•°æ®å¢å¼ºå¯åŠ¨é˜ˆå€¼
        self.aug_enabled = False  # åˆå§‹ä¸å¯ç”¨æ•°æ®å¢å¼º
        self.high_acc_count = 0  # è¿ç»­é«˜å‡†ç¡®ç‡è®¡æ•°

        self.epochs = epochs
        self.batch_size = batch_size
        self.lr = learning_rate
        self.optimizer_type = optimizer

        # åˆå§‹åŒ–æ•°æ®å¢å¼ºå™¨ï¼ˆä½†ä¸ç«‹å³ä½¿ç”¨ï¼‰
        if use_augmentation:
            from increase2025 import DataAugmentor
            self.augmenter = DataAugmentor(use_cutmix=use_cutmix,
                                           cutmix_alpha=cutmix_alpha,
                                           cutmix_prob=cutmix_prob)
            print(f"âœ… æ•°æ®å¢å¼ºå·²é…ç½®ï¼Œå°†åœ¨è®­ç»ƒå‡†ç¡®ç‡è¿ç»­3æ¬¡è¾¾åˆ°{aug_start_threshold}åå¯ç”¨")
            if use_cutmix:
                print(f"   CutMixé…ç½® - alpha: {cutmix_alpha}, æ¦‚ç‡: {cutmix_prob}")
        else:
            self.augmenter = None
            print("âŒ æ•°æ®å¢å¼ºå·²ç¦ç”¨")

        # ä¼˜åŒ–å™¨è®¾ç½®
        if optimizer == 'adam':
            self.optimizer = Adam(lr=learning_rate)
        else:
            self.optimizer = None

        self.train_size = x_train.shape[0]
        self.iter_per_epoch = max(self.train_size // batch_size, 1)
        self.max_iter = int(epochs * self.iter_per_epoch)

        self.current_iter = 0
        self.current_epoch = 0

        self.train_loss_list = []
        self.train_acc_list = []
        self.val_acc_list = []

    def augment_batch(self, x_batch, t_batch=None):
        """åŠ¨æ€å¢å¼ºæ‰¹æ¬¡æ•°æ® - æ¯ä¸ªè®­ç»ƒæ­¥éª¤éƒ½ä¼šè°ƒç”¨"""
        if not self.aug_enabled or self.augmenter is None:
            return x_batch, t_batch

        augmented_batch = []

        # åŸºç¡€æ•°æ®å¢å¼ºï¼ˆå•å¼ å›¾åƒï¼‰
        for i in range(x_batch.shape[0]):
            img = x_batch[i]

            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡® (C, H, W) -> (H, W, C) è¿›è¡Œå¢å¼º
            if img.ndim == 3 and img.shape[0] in [1, 3]:  # (C, H, W)
                # è½¬ç½®ä¸º (H, W, C) è¿›è¡Œå¢å¼º
                if img.shape[0] == 3:  # RGB
                    img_hwc = img.transpose(1, 2, 0)
                else:  # ç°åº¦å›¾
                    img_hwc = img[0]  # (1, H, W) -> (H, W)
            else:
                img_hwc = img

            # åå½’ä¸€åŒ–åˆ°0-255èŒƒå›´è¿›è¡Œå¢å¼º
            if img_hwc.max() <= 1.0:
                img_255 = (img_hwc * 255).astype(np.uint8)
            else:
                img_255 = img_hwc.astype(np.uint8)

            try:
                # åº”ç”¨å•å¼ å›¾åƒå¢å¼º
                aug_img = self.augmenter.augment_single_image(img_255)
            except Exception as e:
                print(f"âš ï¸ å›¾åƒå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå›¾: {e}")
                aug_img = img_255  # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå›¾

            # é‡æ–°å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            if img_hwc.max() <= 1.0:
                aug_img = aug_img.astype(np.float32) / 255.0
            else:
                aug_img = aug_img.astype(np.float32)

            # è½¬å›åŸå§‹æ ¼å¼ (H, W, C) -> (C, H, W)
            if img.ndim == 3 and img.shape[0] in [1, 3]:
                if img.shape[0] == 3:  # RGB
                    aug_img = aug_img.transpose(2, 0, 1)
                else:  # ç°åº¦å›¾
                    aug_img = aug_img[np.newaxis, :, :]  # (H, W) -> (1, H, W)

            augmented_batch.append(aug_img)

        x_augmented = np.array(augmented_batch)

        # åº”ç”¨CutMixå¢å¼º
        if self.use_cutmix and t_batch is not None and t_batch.ndim == 2:  # éœ€è¦one-hotæ ‡ç­¾
            try:
                x_augmented, t_batch = self.augmenter.apply_cutmix(x_augmented, t_batch)
            except Exception as e:
                print(f"âš ï¸ CutMixå¢å¼ºå¤±è´¥: {e}")
                # å¦‚æœCutMixå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å¢å¼ºçš„æ•°æ®

        return x_augmented, t_batch

    def check_augmentation_condition(self, train_acc):
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³å¯ç”¨æ•°æ®å¢å¼ºçš„æ¡ä»¶"""
        if not self.use_augmentation or self.aug_enabled:
            return

        # æ£€æŸ¥å‡†ç¡®ç‡æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
        if train_acc >= self.aug_start_threshold:
            self.high_acc_count += 1
            print(f"ğŸ¯ é«˜å‡†ç¡®ç‡è®¡æ•°: {self.high_acc_count}/3 (å½“å‰å‡†ç¡®ç‡: {train_acc:.4f})")
        else:
            self.high_acc_count = 0  # é‡ç½®è®¡æ•°

        # å¦‚æœè¿ç»­3æ¬¡è¾¾åˆ°é˜ˆå€¼ï¼Œå¯ç”¨æ•°æ®å¢å¼º
        if self.high_acc_count >= 3:
            self.aug_enabled = True
            print("ğŸš€ è®­ç»ƒå‡†ç¡®ç‡è¿ç»­3æ¬¡è¾¾åˆ°é˜ˆå€¼ï¼Œç°åœ¨å¯ç”¨æ•°æ®å¢å¼ºï¼")
            print("   è¿™å°†å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°æ³›åŒ–ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©")

    def train_step(self):
        """å•æ¬¡è®­ç»ƒæ­¥éª¤"""
        batch_mask = np.random.choice(self.train_size, self.batch_size)
        x_batch = self.x_train[batch_mask]
        t_batch = self.t_train[batch_mask]

        # åŠ¨æ€æ•°æ®å¢å¼ºï¼ˆä»…åœ¨å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
        if self.aug_enabled and self.augmenter is not None:
            x_batch, t_batch = self.augment_batch(x_batch, t_batch)

        # è®¡ç®—æ¢¯åº¦
        grads = self.network.gradient(x_batch, t_batch)

        # æ›´æ–°å‚æ•°
        if self.optimizer_type == 'adam':
            self.optimizer.update(self.network.params, grads)
        else:
            for key in grads.keys():
                self.network.params[key] -= self.lr * grads[key]

        # è®¡ç®—æŸå¤±
        loss = self.network.loss(x_batch, t_batch)#è°ƒç”¨loss->è°ƒç”¨predict,è¿”å›æ—¶è°ƒç”¨forward
        self.train_loss_list.append(loss)

        # æ¯ä¸ªepochç»“æŸæ—¶è®¡ç®—å‡†ç¡®ç‡
        if self.current_iter % self.iter_per_epoch == 0:
            self.current_epoch += 1

            train_acc = self.network.accuracy(self.x_train, self.t_train)
            val_acc = self.network.accuracy(self.x_val, self.t_val)

            self.train_acc_list.append(train_acc)
            self.val_acc_list.append(val_acc)

            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ•°æ®å¢å¼ºå¯ç”¨æ¡ä»¶
            self.check_augmentation_condition(train_acc)

            print(f"=== Epoch {self.current_epoch} ===")
            print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}",end=" ")
            print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}",end=" ")
            print(f"æŸå¤±: {loss:.4f}",end=" ")
            if self.aug_enabled:
                print("ğŸŒŸ æ•°æ®å¢å¼º: å·²å¯ç”¨")
            else:
                print("â³ æ•°æ®å¢å¼º: ç­‰å¾…è§¦å‘")

        self.current_iter += 1

    def train(self):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        print("å¼€å§‹è®­ç»ƒ...")
        print(f"ç½‘ç»œç»“æ„: Conv-BN-Relu-Pool-Conv-BN-Relu-Pool-Affine-BN-Relu-Dropout-Affine")
        print(f"è®­ç»ƒè½®æ¬¡: {self.epochs}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"å­¦ä¹ ç‡: {self.lr}")
        print(f"è®­ç»ƒé›†æ ·æœ¬: {self.train_size}")
        print(f"éªŒè¯é›†æ ·æœ¬: {len(self.x_val)}")
        print(f"ä¼˜åŒ–å™¨: {self.optimizer_type}")
        print(f"ä½¿ç”¨BatchNorm: {self.network.use_batchnorm}")
        print(f"æ•°æ®å¢å¼ºç­–ç•¥: åŠ¨æ€å¯ç”¨ (é˜ˆå€¼: {self.aug_start_threshold})")
        print(f"æœ€ç»ˆä½¿ç”¨CutMix: {self.use_cutmix}\n")

        # é‡ç½®æ—©åœè®¡æ•°å™¨
        self.best_val_acc = 0.0
        self.no_improve_count = 0
        self.best_params = None
        self.aug_enabled = False  # ç¡®ä¿åˆå§‹çŠ¶æ€
        self.high_acc_count = 0

        for i in range(self.max_iter):
            self.train_step()

            # æ¯ä¸ªepochç»“æŸæ—¶æ£€æŸ¥æ—©åœæ¡ä»¶
            if self.current_iter % self.iter_per_epoch == 0 and len(self.val_acc_list) > 0:
                current_val_acc = self.val_acc_list[-1]

                # å¦‚æœå½“å‰éªŒè¯å‡†ç¡®ç‡æ¯”æœ€ä½³å€¼å¥½ï¼Œä¿å­˜æ¨¡å‹å‚æ•°
                if current_val_acc > self.best_val_acc:
                    self.best_val_acc = current_val_acc
                    self.no_improve_count = 0
                    # ä¿å­˜å½“å‰æœ€ä½³å‚æ•°
                    self.best_params = {}
                    for key, val in self.network.params.items():
                        self.best_params[key] = val.copy()
                else:
                    self.no_improve_count += 1

                # æ—©åœæ£€æŸ¥
                if self.no_improve_count >= self.patience and len(self.x_val) > 0:
                    print(f"\næ—©åœ: éªŒè¯å‡†ç¡®ç‡åœ¨ {self.patience} ä¸ªepochå†…æœªæå‡")
                    # æ¢å¤æœ€ä½³å‚æ•°
                    if self.best_params is not None:
                        for key in self.best_params.keys():
                            self.network.params[key] = self.best_params[key].copy()
                    break

        # æœ€ç»ˆè¯„ä¼°
        final_train_acc = self.network.accuracy(self.x_train, self.t_train)
        final_val_acc = self.network.accuracy(self.x_val, self.t_val) if len(self.x_val) > 0 else 0

        print("\n=== è®­ç»ƒå®Œæˆ ===")
        print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}")
        print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.4f}")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        print(f"æ•°æ®å¢å¼ºçŠ¶æ€: {'å·²å¯ç”¨' if self.aug_enabled else 'æœªå¯ç”¨'}")
        if len(self.x_val) > 0:
            print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {final_train_acc - final_val_acc:.4f}")

        return self.train_loss_list, self.train_acc_list, self.val_acc_list

    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        plt.figure(figsize=(12, 4))

        # æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(self.train_loss_list)
        plt.title('Training Loss')
        plt.xlabel('Iterations')
        plt.ylabel('Loss')
        plt.grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 2, 2)
        epochs = range(1, len(self.train_acc_list) + 1)
        plt.plot(epochs, self.train_acc_list, label='Train Accuracy', marker='o')
        if len(self.val_acc_list) > 0:
            plt.plot(epochs, self.val_acc_list, label='Validation Accuracy', marker='s')
        plt.title('Training and Validation Accuracy')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()