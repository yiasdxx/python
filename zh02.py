import numpy as np
from common.util import im2col
from collections import OrderedDict


def im2col(input_data, filter_h, filter_w, stride=1, pad=1):
    """
    Parameters
    ----------
    input_data : ç”±(æ•°æ®é‡, é€šé“, é«˜, é•¿)çš„4ç»´æ•°ç»„æ„æˆçš„è¾“å…¥æ•°æ®
    filter_h : æ»¤æ³¢å™¨çš„é«˜
    filter_w : æ»¤æ³¢å™¨çš„é•¿
    stride : æ­¥å¹…
    pad : å¡«å……

    Returns
    -------
    col : 2ç»´æ•°ç»„
    """
    N, C, H, W = input_data.shape
    out_h = (H + 2 * pad - filter_h) // stride + 1
    out_w = (W + 2 * pad - filter_w) // stride + 1

    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')
    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))

    for y in range(filter_h):
        y_max = y + stride * out_h
        for x in range(filter_w):
            x_max = x + stride * out_w
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]
    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)
    return col


def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=1):
    """
    Parameters
    ----------
    col :
    input_shape : è¾“å…¥æ•°æ®çš„å½¢çŠ¶ï¼ˆä¾‹ï¼š(10, 1, 28, 28)ï¼‰
    filter_h :
    filter_w
    stride
    pad

    Returns
    -------
    """
    N, C, H, W = input_shape
    out_h = (H + 2 * pad - filter_h) // stride + 1
    out_w = (W + 2 * pad - filter_w) // stride + 1
    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)

    img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))
    for y in range(filter_h):
        y_max = y + stride * out_h
        for x in range(filter_w):
            x_max = x + stride * out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]

    return img[:, :, pad:H + pad, pad:W + pad]


def softmax(x):
    """Softmaxå‡½æ•°å®ç°"""
    if x.ndim == 2:
        x = x - x.max(axis=1, keepdims=True)
        x = np.exp(x)
        x /= x.sum(axis=1, keepdims=True)
    elif x.ndim == 1:
        x = x - np.max(x)
        x = np.exp(x) / np.sum(np.exp(x))
    return x


def cross_entropy_error(y, t):
    """äº¤å‰ç†µè¯¯å·® - æ™®é€šå‡½æ•°"""
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]

    # å¦‚æœç›‘ç£æ•°æ®æ˜¯one-hotå‘é‡ï¼Œè½¬æ¢ä¸ºæ­£ç¡®è§£æ ‡ç­¾çš„ç´¢å¼•
    if t.ndim == 2 and t.shape[1] > 1:  # one-hotç¼–ç 
        t = t.argmax(axis=1)

    # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°ç±»å‹çš„ä¸€ç»´æ•°ç»„
    t = t.astype(np.int64)
    if t.ndim > 1:
        t = t.flatten()

    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size


def relu(x):
    """ReLUå‡½æ•°"""
    return np.maximum(0, x)


def relu_grad(x):
    """ReLUå‡½æ•°çš„æ¢¯åº¦"""
    grad = np.zeros_like(x)
    grad[x > 0] = 1
    return grad


# Batch Normalizationå±‚
class BatchNormalization:
    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None
        self.reshape_from_4d = False
        self.original_4d_shape = None
        self.running_mean = running_mean
        self.running_var = running_var
        self.batch_size = None
        self.xc = None
        self.xn = None
        self.std = None
        self.dgamma = None
        self.dbeta = None

    def forward(self, x, train_flg=True):

        self.input_shape = x.shape
        self.reshape_from_4d = False

        # å¤„ç†4ç»´å·ç§¯æ•°æ®
        if x.ndim == 4:
            N, C, H, W = x.shape

            self.reshape_from_4d = True
            self.original_4d_shape = (N, C, H, W)

            x_transposed = x.transpose(0, 2, 3, 1)

            x_reshaped = x_transposed.reshape(-1, C)

            out_reshaped = self.__forward_2d(x_reshaped, train_flg)

            out = out_reshaped.reshape(N, H, W, C).transpose(0, 3, 1, 2)
            return out
        else:
            return self.__forward_2d(x, train_flg)

    def __forward_2d(self, x, train_flg):

        if self.running_mean is None:
            D = x.shape[1]
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)


        if train_flg:
            mu = x.mean(axis=0)
            xc = x - mu
            var = np.mean(xc ** 2, axis=0)
            std = np.sqrt(var + 1e-7)
            xn = xc / std

            self.batch_size = x.shape[0]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var
        else:
            xc = x - self.running_mean
            std = np.sqrt(self.running_var + 1e-7)
            xn = xc / std

        out = self.gamma * xn + self.beta


        return out

    def backward(self, dout):

        if self.reshape_from_4d:
            N, C, H, W = self.original_4d_shape
            dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, C)
        else:
            dout_reshaped = dout.reshape(self.xn.shape if self.xn is not None else dout.shape)

        dx_reshaped = self.__backward_2d(dout_reshaped)

        if self.reshape_from_4d:
            N, C, H, W = self.original_4d_shape
            dx = dx_reshaped.reshape(N, H, W, C).transpose(0, 3, 1, 2)
        else:
            dx = dx_reshaped.reshape(*self.input_shape)

        return dx

    def __backward_2d(self, dout):

        if self.batch_size is None:
            self.batch_size = dout.shape[0]

        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xn * dout, axis=0)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)
        dvar = 0.5 * dstd / self.std
        dxc += (2.0 / self.batch_size) * self.xc * dvar
        dmu = np.sum(dxc, axis=0)
        dx = dxc - dmu / self.batch_size

        self.dgamma = dgamma
        self.dbeta = dbeta
        return dx


# ç½‘ç»œå±‚
class Convolution:
    def __init__(self, W, b, stride=1, pad=1):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
        self.x = None
        self.col = None
        self.col_W = None
        self.dW = None
        self.db = None

    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        OH = int(1 + (H + 2 * self.pad - FH) / self.stride)
        OW = int(1 + (W + 2 * self.pad - FW) / self.stride)

        col = im2col(x, FH, FW, self.stride, self.pad)
        col_w = self.W.reshape(FN, -1).T
        out = np.dot(col, col_w) + self.b
        out = out.reshape(N, OH, OW, -1).transpose((0, 3, 1, 2))

        self.x = x
        self.col = col
        self.col_W = col_w

        return out

    def backward(self, dout):
        FN, C, FH, FW = self.W.shape
        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)

        self.db = np.sum(dout, axis=0)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)

        dcol = np.dot(dout, self.col_W.T)
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)

        return dx


class Pooling:
    def __init__(self, pool_h, pool_w, stride, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        self.x = None
        self.arg_max = None

    def forward(self, x):
        N, C, H, W = x.shape
        OH = int(1 + (H - self.pool_h) / self.stride)
        OW = int(1 + (W - self.pool_w) / self.stride)

        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h * self.pool_w)

        arg_max = np.argmax(col, axis=1)
        out = np.max(col, axis=1)

        out = out.reshape(N, OH, OW, C).transpose((0, 3, 1, 2))
        self.x = x
        self.arg_max = arg_max
        return out

    def backward(self, dout):
        dout = dout.transpose(0, 2, 3, 1)

        pool_size = self.pool_h * self.pool_w
        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,))

        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)

        return dx


class Adam:
    """ä¿®æ­£çš„Adamä¼˜åŒ–å™¨ - æ·»åŠ åç½®æ ¡æ­£"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.iter = 0
        self.m = None
        self.v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1

        for key in params.keys():
            if key in grads:
                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)

                m_hat = self.m[key] / (1 - self.beta1 ** self.iter)
                v_hat = self.v[key] / (1 - self.beta2 ** self.iter)

                params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)


class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.original_shape = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.original_shape = x.shape
        x = x.reshape(x.shape[0], -1)
        self.x = x
        out = np.dot(self.x, self.W) + self.b
        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        dx = dx.reshape(self.original_shape)
        return dx

class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)

    def backward(self, dout):
        return dout * self.mask

class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x > 0)
        return np.maximum(x, 0)

    def backward(self, dout):
        dx = dout * self.mask
        return dx


class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)

        # å¤„ç†one-hotç¼–ç æ ‡ç­¾
        if self.t.ndim == 2 and self.t.shape[1] > 1:
            self.t = self.t.argmax(axis=1)

        # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°ç±»å‹çš„ä¸€ç»´æ•°ç»„
        self.t = self.t.astype(np.int64)
        if self.t.ndim > 1:
            self.t = self.t.flatten()

        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = self.y.copy()
        dx[np.arange(batch_size), self.t] -= 1
        dx = dx / batch_size
        return dx


# å…·ä½“ç½‘ç»œæ¨¡å‹
class SimpleConvNet:
    """é’ˆå¯¹å¤§å°ºå¯¸è¾“å…¥çš„å·ç§¯ç¥ç»ç½‘ç»œ - å…¨è¿æ¥å±‚ä¹Ÿä½¿ç”¨BN"""

    def __init__(self, input_dim=(3, 64, 64), dropout_ratio=0.5,
                 conv1_param={'filter_num': 16, 'filter_size': 3, 'pad': 1, 'stride': 1},
                 conv2_param={'filter_num': 32, 'filter_size': 3, 'pad': 1, 'stride': 1},
                 conv3_param={'filter_num': 64, 'filter_size': 3, 'pad': 1, 'stride': 1},
                 hidden_size=100, output_size=10, use_batchnorm=False):

        self.dropout_ratio = dropout_ratio
        self.use_batchnorm = use_batchnorm

        # ç¬¬ä¸€å±‚å·ç§¯å‚æ•°
        filter_num1 = conv1_param['filter_num']
        filter_size1 = conv1_param['filter_size']
        filter_pad1 = conv1_param['pad']
        filter_stride1 = conv1_param['stride']

        # ç¬¬äºŒå±‚å·ç§¯å‚æ•°
        filter_num2 = conv2_param['filter_num']
        filter_size2 = conv2_param['filter_size']
        filter_pad2 = conv2_param['pad']
        filter_stride2 = conv2_param['stride']

        # ç¬¬ä¸‰å±‚å·ç§¯å‚æ•°
        filter_num3 = conv3_param['filter_num']
        filter_size3 = conv3_param['filter_size']
        filter_pad3 = conv3_param['pad']
        filter_stride3 = conv3_param['stride']

        input_size = input_dim[1]

        # è®¡ç®—å„å±‚è¾“å‡ºå°ºå¯¸
        conv1_output_size = (input_size - filter_size1 + 2 * filter_pad1) // filter_stride1 + 1
        pool1_output_size = conv1_output_size // 2
        conv2_output_size = (pool1_output_size - filter_size2 + 2 * filter_pad2) // filter_stride2 + 1
        pool2_output_size = conv2_output_size // 2
        conv3_output_size = (pool2_output_size - filter_size3 + 2 * filter_pad3) // filter_stride3 + 1
        pool3_output_size = conv3_output_size // 2
        fc_input_size = filter_num3 * pool3_output_size * pool3_output_size

        print(f"ç½‘ç»œç»“æ„è®¡ç®—:")
        print(f"è¾“å…¥: {input_dim}")
        print(f"å·ç§¯1è¾“å‡º: {filter_num1} x {conv1_output_size} x {conv1_output_size}")
        print(f"æ± åŒ–1è¾“å‡º: {filter_num1} x {pool1_output_size} x {pool1_output_size}")
        print(f"å·ç§¯2è¾“å‡º: {filter_num2} x {conv2_output_size} x {conv2_output_size}")
        print(f"æ± åŒ–2è¾“å‡º: {filter_num2} x {pool2_output_size} x {pool2_output_size}")
        print(f"å·ç§¯3è¾“å‡º: {filter_num3} x {conv3_output_size} x {conv3_output_size}")
        print(f"æ± åŒ–3è¾“å‡º: {filter_num3} x {pool3_output_size} x {pool3_output_size}")
        print(f"å…¨è¿æ¥å±‚è¾“å…¥: {fc_input_size}")
        print(f"ä½¿ç”¨Batch Normalization: {use_batchnorm}")

        # åˆå§‹åŒ–æƒé‡
        self.params = {}

        # å·ç§¯å±‚æƒé‡
        he_std_conv1 = np.sqrt(2.0 / (input_dim[0] * filter_size1 * filter_size1))
        self.params['W1'] = he_std_conv1 * np.random.randn(filter_num1, input_dim[0], filter_size1, filter_size1)
        self.params['b1'] = np.zeros(filter_num1)

        he_std_conv2 = np.sqrt(2.0 / (filter_num1 * filter_size2 * filter_size2))
        self.params['W2'] = he_std_conv2 * np.random.randn(filter_num2, filter_num1, filter_size2, filter_size2)
        self.params['b2'] = np.zeros(filter_num2)

        # ç¬¬ä¸‰å±‚å·ç§¯æƒé‡
        he_std_conv3 = np.sqrt(2.0 / (filter_num2 * filter_size3 * filter_size3))
        self.params['W3'] = he_std_conv3 * np.random.randn(filter_num3, filter_num2, filter_size3, filter_size3)
        self.params['b3'] = np.zeros(filter_num3)

        # å…¨è¿æ¥å±‚æƒé‡
        he_std_fc1 = np.sqrt(2.0 / fc_input_size)
        self.params['W4'] = he_std_fc1 * np.random.randn(fc_input_size, hidden_size)
        self.params['b4'] = np.zeros(hidden_size)

        he_std_fc2 = np.sqrt(2.0 / hidden_size)
        self.params['W5'] = he_std_fc2 * np.random.randn(hidden_size, output_size)
        self.params['b5'] = np.zeros(output_size)

        # Batch Normalization å‚æ•° - ä¸ºæ‰€æœ‰å±‚æ·»åŠ BNå‚æ•°
        if self.use_batchnorm:
            # å·ç§¯å±‚çš„BNå‚æ•°
            self.params['gamma1'] = np.ones(filter_num1)
            self.params['beta1'] = np.zeros(filter_num1)
            self.params['gamma2'] = np.ones(filter_num2)
            self.params['beta2'] = np.zeros(filter_num2)
            self.params['gamma3'] = np.ones(filter_num3)
            self.params['beta3'] = np.zeros(filter_num3)

            # å…¨è¿æ¥å±‚çš„BNå‚æ•°ï¼ˆæ–°å¢ï¼‰
            self.params['gamma4'] = np.ones(hidden_size)  # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚åçš„BN
            self.params['beta4'] = np.zeros(hidden_size)

        # ç”Ÿæˆå±‚
        self.layers = OrderedDict()

        # ç¬¬ä¸€å·ç§¯å—
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],
                                           conv1_param['stride'], conv1_param['pad'])
        if self.use_batchnorm:
            self.layers['BatchNorm1'] = BatchNormalization(self.params['gamma1'], self.params['beta1'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)

        # ç¬¬äºŒå·ç§¯å—
        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'],
                                           conv2_param['stride'], conv2_param['pad'])
        if self.use_batchnorm:
            self.layers['BatchNorm2'] = BatchNormalization(self.params['gamma2'], self.params['beta2'])
        self.layers['Relu2'] = Relu()
        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)

        # ç¬¬ä¸‰å·ç§¯å—
        self.layers['Conv3'] = Convolution(self.params['W3'], self.params['b3'],
                                           conv3_param['stride'], conv3_param['pad'])
        if self.use_batchnorm:
            self.layers['BatchNorm3'] = BatchNormalization(self.params['gamma3'], self.params['beta3'])
        self.layers['Relu3'] = Relu()
        self.layers['Pool3'] = Pooling(pool_h=2, pool_w=2, stride=2)

        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å—ï¼ˆæ·»åŠ BNå±‚ï¼‰
        self.layers['Affine1'] = Affine(self.params['W4'], self.params['b4'])
        if self.use_batchnorm:
            self.layers['BatchNorm4'] = BatchNormalization(self.params['gamma4'], self.params['beta4'])
        self.layers['Relu4'] = Relu()
        self.layers['Dropout1'] = Dropout(dropout_ratio)

        # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ï¼ˆè¾“å‡ºå±‚ï¼Œé€šå¸¸ä¸åŠ BNï¼‰
        self.layers['Affine2'] = Affine(self.params['W5'], self.params['b5'])

        self.last_layer = SoftmaxWithLoss()
    def predict(self, x, train_flg=False):
        """å‰å‘ä¼ æ’­é¢„æµ‹

        Args:
            x: è¾“å…¥æ•°æ®
            train_flg: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå½±å“BNå±‚å’ŒDropoutå±‚è¡Œä¸ºï¼‰
        """
        for key, layer in self.layers.items():
            if 'BatchNorm' in key or 'Dropout' in key:
                # BNå±‚å’ŒDropoutå±‚éœ€è¦train_flgå‚æ•°
                x = layer.forward(x, train_flg)
            else:
                x = layer.forward(x)
        return x

    def loss(self, x, t):
        """è®¡ç®—æŸå¤±"""
        y = self.predict(x, train_flg=True)
        return self.last_layer.forward(y, t)

    def accuracy(self, x, t, batch_size=100):
        """è®¡ç®—å‡†ç¡®ç‡"""
        if t.ndim != 1:
            t = np.argmax(t, axis=1)

        acc = 0.0
        for i in range(0, x.shape[0], batch_size):
            tx = x[i:i + batch_size]
            tt = t[i:i + batch_size]
            y = self.predict(tx, train_flg=False)
            y = np.argmax(y, axis=1)
            acc += np.sum(y == tt)

        return acc / x.shape[0]

    def gradient(self, x, t):
        """è®¡ç®—æ¢¯åº¦"""
        self.loss(x, t)

        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        grads = {}
        grads['W1'] = self.layers['Conv1'].dW
        grads['b1'] = self.layers['Conv1'].db
        grads['W2'] = self.layers['Conv2'].dW
        grads['b2'] = self.layers['Conv2'].db
        grads['W3'] = self.layers['Conv3'].dW
        grads['b3'] = self.layers['Conv3'].db
        grads['W4'] = self.layers['Affine1'].dW
        grads['b4'] = self.layers['Affine1'].db
        grads['W5'] = self.layers['Affine2'].dW
        grads['b5'] = self.layers['Affine2'].db

        if self.use_batchnorm:
            grads['gamma1'] = self.layers['BatchNorm1'].dgamma
            grads['beta1'] = self.layers['BatchNorm1'].dbeta
            grads['gamma2'] = self.layers['BatchNorm2'].dgamma
            grads['beta2'] = self.layers['BatchNorm2'].dbeta
            grads['gamma3'] = self.layers['BatchNorm3'].dgamma
            grads['beta3'] = self.layers['BatchNorm3'].dbeta
            grads['gamma4'] = self.layers['BatchNorm4'].dgamma  # æ–°å¢å…¨è¿æ¥å±‚BNæ¢¯åº¦
            grads['beta4'] = self.layers['BatchNorm4'].dbeta  # æ–°å¢å…¨è¿æ¥å±‚BNæ¢¯åº¦

        return grads

# è®­ç»ƒå™¨
# åœ¨ Trainer ç±»ä¸­æ·»åŠ æ•°æ®å¢å¼ºåŠŸèƒ½
class Trainer:
    """åªæœ‰è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„è®­ç»ƒå™¨"""

    def __init__(self, network, x_train, t_train, x_val, t_val,
                 epochs=20, batch_size=100, optimizer='adam', learning_rate=0.001,
                 augmentor=None, enable_augmentation_threshold=0.8):  # æ–°å¢å‚æ•°
        self.network = network
        self.x_train = x_train
        self.t_train = t_train
        self.x_val = x_val
        self.t_val = t_val

        self.epochs = epochs
        self.batch_size = batch_size
        self.lr = learning_rate
        self.optimizer_type = optimizer

        # æ•°æ®å¢å¼ºç›¸å…³å±æ€§
        self.augmentor = augmentor
        self.enable_augmentation_threshold = enable_augmentation_threshold
        self.augmentation_enabled = False
        self.high_accuracy_count = 0
        self.last_train_accuracies = []

        if optimizer == 'adam':
            self.optimizer = Adam(lr=learning_rate)
        else:
            self.optimizer = None

        self.train_size = x_train.shape[0]
        self.iter_per_epoch = max(self.train_size // batch_size, 1)
        self.max_iter = int(epochs * self.iter_per_epoch)

        self.current_iter = 0
        self.current_epoch = 0

        self.train_loss_list = []
        self.train_acc_list = []
        self.val_acc_list = []

    def check_and_enable_augmentation(self, current_train_acc):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨æ•°æ®å¢å¼º"""
        if self.augmentor is None or self.augmentation_enabled:
            return

        # è®°å½•æœ€è¿‘çš„å‡†ç¡®ç‡
        self.last_train_accuracies.append(current_train_acc)
        if len(self.last_train_accuracies) > 3:
            self.last_train_accuracies.pop(0)

        # æ£€æŸ¥æ˜¯å¦è¿ç»­ä¸‰æ¬¡è¾¾åˆ°é˜ˆå€¼
        if len(self.last_train_accuracies) == 3:
            if all(acc >= self.enable_augmentation_threshold for acc in self.last_train_accuracies):
                self.augmentation_enabled = True
                print(f"\nğŸ¯ è®­ç»ƒå‡†ç¡®ç‡è¿ç»­ä¸‰æ¬¡è¾¾åˆ° {self.enable_augmentation_threshold}ï¼Œå¯ç”¨æ•°æ®å¢å¼ºï¼")
                print(f"æœ€è¿‘ä¸‰æ¬¡å‡†ç¡®ç‡: {self.last_train_accuracies}")

    def prepare_training_batch(self, batch_mask):
        """å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡ï¼Œåº”ç”¨æ•°æ®å¢å¼º"""
        x_batch = self.x_train[batch_mask]
        t_batch = self.t_train[batch_mask]

        # å¦‚æœå¯ç”¨äº†æ•°æ®å¢å¼ºä¸”å¢å¼ºå™¨å¯ç”¨
        if self.augmentation_enabled and self.augmentor is not None:
            try:
                # å°†æ•°æ®è½¬æ¢ä¸ºé€‚åˆå¢å¼ºçš„æ ¼å¼ (N, C, H, W) -> (N, H, W, C)
                if x_batch.ndim == 4 and x_batch.shape[1] in [1, 3]:  # é€šé“åœ¨å‰æ ¼å¼
                    x_batch_aug = x_batch.transpose(0, 2, 3, 1)
                else:
                    x_batch_aug = x_batch.copy()

                # åå½’ä¸€åŒ–åˆ°0-255èŒƒå›´è¿›è¡Œå¢å¼º
                if x_batch_aug.max() <= 1.0:
                    x_batch_aug = (x_batch_aug * 255).astype(np.uint8)

                # åº”ç”¨å¢å¼ºåˆ°æ¯ä¸ªå›¾åƒ
                augmented_batch = []
                for i in range(len(x_batch_aug)):
                    augmented_img = self.augmentor.augment_single_image(x_batch_aug[i])
                    augmented_batch.append(augmented_img)

                x_batch_aug = np.array(augmented_batch)

                # é‡æ–°å½’ä¸€åŒ–å¹¶è½¬æ¢å›åŸå§‹æ ¼å¼
                x_batch_aug = x_batch_aug.astype(np.float32) / 255.0
                if x_batch.ndim == 4 and x_batch.shape[1] in [1, 3]:
                    x_batch_aug = x_batch_aug.transpose(0, 3, 1, 2)

                return x_batch_aug, t_batch

            except Exception as e:
                print(f"æ•°æ®å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                return x_batch, t_batch

        return x_batch, t_batch

    def train_step(self):
        """å•æ¬¡è®­ç»ƒæ­¥éª¤"""
        batch_mask = np.random.choice(self.train_size, self.batch_size)

        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®ï¼ˆå¯èƒ½åº”ç”¨å¢å¼ºï¼‰
        x_batch, t_batch = self.prepare_training_batch(batch_mask)

        # è®¡ç®—æ¢¯åº¦
        grads = self.network.gradient(x_batch, t_batch)

        # æ›´æ–°å‚æ•°
        if self.optimizer_type == 'adam':
            self.optimizer.update(self.network.params, grads)
        else:
            for key in grads.keys():
                self.network.params[key] -= self.lr * grads[key]

        # è®¡ç®—æŸå¤±
        loss = self.network.loss(x_batch, t_batch)
        self.train_loss_list.append(loss)

        # æ¯ä¸ªepochç»“æŸæ—¶è®¡ç®—å‡†ç¡®ç‡
        if self.current_iter % self.iter_per_epoch == 0:
            self.current_epoch += 1

            train_acc = self.network.accuracy(self.x_train, self.t_train)
            val_acc = self.network.accuracy(self.x_val, self.t_val)

            self.train_acc_list.append(train_acc)
            self.val_acc_list.append(val_acc)

            # æ£€æŸ¥å¹¶å¯èƒ½å¯ç”¨æ•°æ®å¢å¼º
            self.check_and_enable_augmentation(train_acc)

            print(f"=== Epoch {self.current_epoch} ===")
            print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}")
            print(f"æŸå¤±: {loss:.4f}")
            if self.augmentation_enabled:
                print("ğŸ”§ æ•°æ®å¢å¼º: å·²å¯ç”¨")
            else:
                print(f"æ•°æ®å¢å¼º: æœªå¯ç”¨ (éœ€è¦è¿ç»­3æ¬¡å‡†ç¡®ç‡ â‰¥ {self.enable_augmentation_threshold})")

        self.current_iter += 1

    def train(self):
        """å®Œæ•´è®­ç»ƒ"""
        print("å¼€å§‹è®­ç»ƒ...")
        print(f"ç½‘ç»œç»“æ„: Conv-Relu-Pool-Conv-Relu-Pool-Affine-Relu-Affine")
        print(f"æ¿€æ´»å‡½æ•°: ReLU")
        print(f"æƒé‡åˆå§‹åŒ–: Heåˆå§‹åŒ–")
        print(f"è®­ç»ƒè½®æ¬¡: {self.epochs}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"å­¦ä¹ ç‡: {self.lr}")
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {self.train_size}")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°: {self.x_val.shape[0]}")
        print(f"ä½¿ç”¨ä¼˜åŒ–å™¨: {self.optimizer_type}")
        print(f"ä½¿ç”¨BatchNorm: {self.network.use_batchnorm}")
        print(f"æ•°æ®å¢å¼ºé˜ˆå€¼: è¿ç»­3æ¬¡è®­ç»ƒå‡†ç¡®ç‡ â‰¥ {self.enable_augmentation_threshold}")
        print(f"æ•°æ®å¢å¼ºå™¨: {'å·²é…ç½®' if self.augmentor is not None else 'æœªé…ç½®'}\n")

        for i in range(self.max_iter):
            self.train_step()

        # æœ€ç»ˆè¯„ä¼°
        final_train_acc = self.network.accuracy(self.x_train, self.t_train)
        final_val_acc = self.network.accuracy(self.x_val, self.t_val)

        print("=== è®­ç»ƒå®Œæˆ ===")
        print(f"æœ€ç»ˆè®­ç»ƒé›†å‡†ç¡®ç‡: {final_train_acc:.4f}")
        print(f"æœ€ç»ˆéªŒè¯é›†å‡†ç¡®ç‡: {final_val_acc:.4f}")
        print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {final_train_acc - final_val_acc:.4f}")
        if self.augmentation_enabled:
            print("æ•°æ®å¢å¼ºçŠ¶æ€: å·²å¯ç”¨")
        else:
            print("æ•°æ®å¢å¼ºçŠ¶æ€: æœªå¯ç”¨")

        return self.train_loss_list, self.train_acc_list, self.val_acc_list

    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        import matplotlib.pyplot as plt

        plt.figure(figsize=(12, 4))

        # æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(self.train_loss_list)
        plt.title('Training Loss')
        plt.xlabel('Iterations')
        plt.ylabel('Loss')
        plt.grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 2, 2)
        epochs = range(1, len(self.train_acc_list) + 1)
        plt.plot(epochs, self.train_acc_list, label='Train Accuracy', marker='o')
        plt.plot(epochs, self.val_acc_list, label='Validation Accuracy', marker='s')
        plt.title('Training and Validation Accuracy')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()